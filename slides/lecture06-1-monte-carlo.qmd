---
title: "Monte Carlo: Why It Works"
subtitle: "Lecture 10"
author: "Vivek Srikrishnan"
course: "BEE 4750"
institution: "Cornell University"
date: "September 30, 2024"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: mathjax
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        auto-stretch: false
        controls: true
engine: julia
julia:
    exeflags: ["+1.10.4"]
execute:
    freeze: auto
    daemon: 600
---

```{julia}
#| output: false
import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false
using Random
using CSV
using DataFrames
using Plots
using Measures
using Distributions
using StatsPlots
using LaTeXStrings


Random.seed!(1)
plot_font = "Computer Modern"
default(
    fontfamily=plot_font,
    linewidth=2, 
    framestyle=:box, 
    label=nothing, 
    grid=false,
    tickfontsize=16,
    guidefontsize=18,
    legendfontsize=16
)
```

# Announcements

## Prelim: Alternative Testing Program

- Conflict/accomodation exams will be handled through ATP.
- **If you do not have an SDS accomodation letter for this class, you will not be enrolled in ATP**.

# Review of Previous Classes

## Uncertainty and Systems

- Systems models always neglect aspects of the "real" system, resulting in uncertainties
- May be external forcings that are unknown.
- Risk: What are the possible impacts of environmental conditions and how probable are they?

## Monte Carlo Simulation

Monte Carlo is **stochastic simulation**.

```{dot}
//| fig-width: 100%
digraph G {
    graph [
        rankdir=LR
        layout=dot
    ]
    node [
        fontname = "IBM Plex Sans, sans-serif"
        fontsize=25
    ]
    edge [
        arrowsize=0.75
        labeldistance=3
        penwidth=3
        fontname = "IBM Plex Sans, sans-serif"
        fontsize=25
        style=dashed
        color="#b31b1b"
        fontcolor="#b31b1b"
    ]
    a [label="Probability\n Distribution"]
    b [label = "Random\n Samples"]
    c [label="Model"]
    d [label="Outputs"]

    a -> b [
        label="Sample"
    ]
    b -> c [
        label="Input"
    ]
    c -> d [
        label="Simulate"
    ]
}

```

## Goals of Monte Carlo

Monte Carlo is a broad method, which can be used to:

1. Obtain probability distributions of outputs (***uncertainty propagation***);
2. Estimate deterministic quantities (***Monte Carlo estimation***).

## Monte Carlo Estimation

Monte Carlo estimation involves framing the (deterministic) quantity of interest as a summary statistic of a random process.

## Questions?

{{< include _poll-prompt.qmd >}}

# Monte Carlo: More Formally

## Why Monte Carlo Works

We can formalize Monte Carlo estimation as the computation of the expected value of a random quantity $Y$, $\mu = \mathbb{E}[Y]$.

To do this, generate $n$ independent and identically distributed values $Y_1, \ldots, Y_n$.  Then the sample estimate is

$$\tilde{\mu}_n = \frac{1}{n}\sum_{i=1}^n Y_i$$

## Monte Carlo (Formally)

**Important note**: The Monte Carlo sample mean $\tilde{\mu}_n = \frac{1}{n}\sum_{i=1}^n Y_i$ **is itself a random variable**.

## What Do We Mean By "Works"?

What properties would we like the Monte Carlo estimate $\mathbb{E}[\tilde{\mu}_n]$ to have?

## Monte Carlo Estimate is Unbiased

With some assumptions (the mean of $Y$ exists and $Y$ has finite variance), the expected Monte Carlo sample mean $\mathbb{E}[\tilde{\mu}_n]$ is

$$\frac{1}{n}\sum_{i=1}^n \mathbb{E}[Y_i] = \frac{1}{n} n \mu = \mu$$

So the Monte Carlo estimate is an *unbiased* estimate of the mean.

## Monte Carlo Estimate Converges

On average (across different samples), the MC estimate $\tilde{\mu}_n$ has the "right" value.

Will a single sample trajectory converge to the true value?

## The Law of Large Numbers

If 

(1) $Y$ is a random variable and its expectation exists and 

(2) $Y_1, \ldots, Y_n$ are independently and identically distributed

Then by the **weak law of large numbers**:

$$\lim_{n \to \infty} \mathbb{P}\left(\left|\tilde{\mu}_n - \mu\right| \leq \varepsilon \right) = 1$$

## Monte Carlo Estimate Converges

In other words, *eventually* Monte Carlo estimates will get within an arbitrary error of the true expectation. But how large is large enough?

Note that the law of large numbers applies to vector-valued functions as well. The key is that $f(x) = Y$ just needs to be sufficiently well-behaved.

## Monte Carlo Error Can Be Estimated

We'd like to know more about the error of this estimate for a given sample size. The variance of this estimator is

$$\tilde{\sigma}_n^2 = \text{Var}\left(\tilde{\mu}_n\right) = \mathbb{E}\left((\tilde{\mu}_n - \mu)^2\right) = \frac{\sigma_Y^2}{n}$$

::: {.fragment .fade-in}
So as $n$ increases, the *standard error* decreases:

$$\tilde{\sigma}_n = \frac{\sigma_Y}{\sqrt{n}}$$
:::

## Monte Carlo Error

In other words, if we want to decrease the Monte Carlo error by 10x, we need 100x additional samples. **This is not an ideal method for high levels of accuracy.** 

::: {.fragment .fade-in}
::: {.quote}
> Monte Carlo is an extremely bad method. It should only be used when all alternative methods are worse.

::: {.cite}
--- Sokal, *Monte Carlo Methods in Statistical Mechanics*, 1996
:::
:::
:::

::: {.fragment .fade-in}

But...often most alternatives *are* worse!
:::



## When Might We Want to Use Monte Carlo?

::: {.fragment .fade-in}
- All models are wrong, and so there always exists some irreducible model error. Can we reduce the Monte Carlo error enough so it's less than the model error and other uncertainties?
- We often need a lot of simulations. Do we have enough computational power?
:::

## When Might We Want to Use Monte Carlo?

If you can compute your answers analytically, you probably should. 

But for *many* systems problems, this is either

1. Not possible;
2. Requires a lot of stylization and simplification.

## Monte Carlo Confidence Intervals

This error estimate lets us compute confidence intervals for the MC estimate.

## What is a Confidence Interval?

**Remember**: an $\alpha$-confidence interval is an interval such that $\alpha \%$ of intervals constructed after a given experiment will contain the true value.

::: {.fragment .fade-in}

It is **not** an interval which contains the true value $\alpha \%$ of the time. This concept does not exist within frequentist statistics, and this mistake is often made.
:::

## How To Interpret Confidence Intervals

:::: {.columns}
::: {.column width=65%}
To understand confidence intervals, think of horseshoes! 

The post is a fixed target, and my accuracy informs how confident I am that I will hit the target with any given toss.

:::
::: {.column width=35%}

![Cartoon of horseshoes](https://www.wikihow.com/images/thumb/2/20/Throw-a-Horseshoe-Step-4-Version-4.jpg/aid448076-v4-728px-Throw-a-Horseshoe-Step-4-Version-4.jpg.webp)

::: {.caption}
Source: <https://www.wikihow.com/Throw-a-Horseshoe>
:::
:::
::::

## How To Interpret Confidence Intervals

**But once I make the throw, I've either hit or missed.**

The confidence level $\alpha\%$ expresses the *pre-experimental* frequency by which a confidence interval will contain the true value. So for a 95% confidence interval, there is a 5% chance that a given sample was an outlier and the interval is inaccurate.


## Monte Carlo Confidence Intervals

OK, back to Monte Carlo...

**Basic Idea**: The *Central Limit Theorem* says that with enough samples, the errors are normally distributed:

$$\left\|\tilde{\mu}_n - \mu\right\| \to \mathcal{N}\left(0, \frac{\sigma_Y^2}{n}\right)$$

## Monte Carlo Confidence Intervals

The $\alpha$-confidence interval is:
$$\tilde{\mu}_n \pm \Phi^{-1}\left(1 - \frac{\alpha}{2}\right) \frac{\sigma_Y}{\sqrt{n}}$$

For example, the 95% confidence interval is $$\tilde{\mu}_n \pm 1.96 \frac{\sigma_Y}{\sqrt{n}}.$$

## Implications of Monte Carlo Error

Converging at a rate of $1/\sqrt{n}$ is not great. But:

- All models are wrong, and so there always exists some irreducible model error. 
- We often need a lot of simulations. Do we have enough computational power?

## Implications of Monte Carlo Error

If you can compute your answer analytically, you probably should. 

But often this is difficult if not impossible without many simplifying assumptions.

# Examples

## MC Example: Dice

What is the probability of rolling 4 dice for a total of 19?

```{julia}
#| echo: true
#| output: false
#| code-line-numbers: "|14-17|18-20|22-25"

function dice_roll_repeated(n_trials, n_dice)
    dice_dist = DiscreteUniform(1, 6) 
	roll_results = zeros(n_trials)
	for i=1:n_trials
		roll_results[i] = sum(rand(dice_dist, n_dice))
	end
	return roll_results
end

nsamp = 10_000
# roll four dice 10,000 times
rolls = dice_roll_repeated(nsamp, 4) 

# initialize storage for frequencies by sample length
avg_freq = zeros(length(rolls)) 
# initialize storage for standard error of estimate
std_freq = zeros(length(rolls))
# compute average frequencies of 19
avg_freq[1] = (rolls[1] == 19)
std_freq[1] = 0 # no standard error for the first sample

for i in 2:length(rolls)
    avg_freq[i] = (avg_freq[i-1] * (i-1) + (rolls[i] == 19)) / i
    std_freq[i] = std(rolls[1:i] .== 19) / sqrt(i)
end
```

## MC Example: Dice

```{julia}
#| echo: true
#| code-fold: true

plt = plot(
    avg_freq, ribbon=1.96 * std_freq,
    xlim = (1, nsamp),
    ylim = (0, 0.1),
    legend = :false,
    xlabel="Iteration",
    ylabel="Estimate",
    left_margin=10mm,
    bottom_margin=10mm,
    right_margin=10mm,
    color=:black,
    linewidth=3
)
plot!(size=(1200, 500))

hline!(plt, [0.0432], color="red", 
    linestyle=:dash) 

```

## MC Example: Dice

After 10,000 iterations, the 95% confidence interval for the MC estimate is `{julia} round(100 * avg_freq[end]; digits=1)` $\pm$ `{julia} round(196 * std_freq[end]; digits=1)`%.

**Is this good enough? Should we keep going?**

## MC Example: Dissolved Oxygen

Let's revisit our two-waste dissolved oxygen example with uncertain wastewater inflows.

![Schematic for Multiple Discharge Example](figures/do-multi-release.svg){width=75%}

## MC Example: Dissolved Oxygen

:::: {.columns}
::: {.column width=50%}
At each inflow, the uncertainty about the amount of waste (which modifies the baseline CBOD/NBOD):
:::
::: {.column width=50%}
```{julia}
#| echo: true
#| code-fold: true
#| label: fig-waste-distribution
#| fig-cap: "Distribution for the uncertainty in waste at each inflow"

waste_dist = truncated(Normal(0, 5); lower=-10, upper=20)
waste_samps = rand(waste_dist, (10_000, 2))
histogram(waste_samps[:, 1], ylabel="Count", xlabel="OD Deviation from Baseline (mg/L)")
plot!(size=(600, 500))
```
:::
::::

## MC Example: Dissolved Oxygen

```{julia}
#| label: fig-do-multi
#| fig-cap: "Multi-Release Dissolved Oxygen Example"
#| fig-width: 100%
#| layout-ncol: 2

function do_simulate(x, C0, B0, N0, ka, kn, kc, Cs, U)
    B = B0 * exp(-kc * x / U)
    N = N0 * exp(-kn * x / U)
    α1 = exp(-ka * x / U)
    α2 = (kc/(ka-kc)) * (exp.(-kc * x / U) - exp(-ka * x / U))
    α3 = (kn/(ka-kn)) * (exp(-kn * x / U) - exp(-ka * x / U))
    C = Cs * (1 - α1) + (C0 * α1) - (B0 * α2) - (N0 * α3)
    return (C, B, N)
end  

ka = 0.6
kc = 0.4
kn = 0.25
C0 = 8
B0 = 4
N0 = 3

Cs = 7
U = 5

Q0 = 100000
Q1 = 20000
Q2 = 15000

x0 = 0
x1 = 18
x2 = 30

Cin1 = 6
Bin1 = 10
Nin1 = 5

Cin2 = 4
Bin2 = 20
Nin2 = 15

function do_mc(w1, w2, x1, x2, Q0, Q1, Q2, C0, B0, N0, Cin1, Bin1, Nin1, Cin2, Bin2, Nin2, ka, kc, kn, Cs, U)

    C01 = (C0 * Q0 + Cin1 * Q1) / (Q0 + Q1)
    B01 = (B0 * Q0 + (Bin1 + w1) * Q1) / (Q0 + Q1)
    N01 = (N0 * Q0 + (Nin1 + w1) * Q1) / (Q0 + Q1)

    do_out1 = (y -> do_simulate.(y, C01, B01, N01, ka, kc, kn, Cs, U))(0:x1)

    C1 = [d[1] for d in do_out1]
    B1 = [d[2] for d in do_out1]
    N1 = [d[3] for d in do_out1]

    C02 = (C1[length(C1)] * (Q0 + Q1) + Cin2 * Q2) / (Q0 + Q1 + Q2)
    B02 = (B1[length(B1)] * (Q0 + Q1) + (Bin2 + w2) * Q2) / (Q0 + Q1 + Q2)
    N02 = (N1[length(N1)] * (Q0 + Q1) + (Nin2 + w2) * Q2) / (Q0 + Q1 + Q2)

    do_out2 = (y -> do_simulate.(y, C02, B02, N02, ka, kc, kn, Cs, U))(0:(x2-x1))

    C2 = [d[1] for d in do_out2]
    B2 = [d[2] for d in do_out2]
    N2 = [d[3] for d in do_out2]

    return [C1[1:end-1]; C2]

end

# find all the minima
C = [minimum(do_mc(waste_samps[i, 1], waste_samps[i, 2], x1, x2, Q0, Q1, Q2, C0, B0, N0, Cin1, Bin1, Nin1, Cin2, Bin2, Nin2, ka, kc, kn, Cs, U)) for i in 1:10_000]

# compute Monte Carlo estimates
C_mc = zeros(length(C))
C_se = zeros(length(C))
C_mc[1] = C[1] .< 4
C_se[1] = 0
for i in 2:length(C)
    C_mc[i] = (((i - 1) * C_mc[i-1]) + (C[i] < 4)) / i
    C_se[i] = std(C[1:i] .< 4) / sqrt(i)
end

p1 = histogram(C, xlabel="Minimum DO Concentration (mg/L)", ylabel="Count", bottom_margin=10mm, left_margin=10mm)
vline!(p1, [4], color=:red, linestyle=:dash, size=(650, 600))
p2 = plot(C_mc, ribbon=1.96 * C_se, color=:black, linewidth=3, ylims=(0, 0.1), xlabel="Iteration", ylabel="Probability of Failing Standard", size=(650, 600), bottom_margin=10mm, left_margin=10mm, right_margin=5mm)
display(p1)
display(p2)
```

## MC Example: Dissolved Oxygen

:::: {.columns}
::: {.column width=50%}
```{julia}
#| label: fig-do-mc
#| fig-cap: "Multi-Release Dissolved Oxygen Example"

plot(p2, right_margin=10mm)
```
:::
::: {.column width=50%}
| Iterations | Estimate |
|:----------:|:--------:|
| 100 | `{julia} round(100 * C_mc[100]; digits=1)` $\pm$ `{julia} round(100 * C_se[100]; digits=1)`% |
| 1000 | `{julia} round(100 * C_mc[1000]; digits=1)` $\pm$ `{julia} round(100 * C_se[1000]; digits=1)`% |
| 5000 | `{julia} round(100 * C_mc[5000]; digits=1)` $\pm$ `{julia} round(100 * C_se[5000]; digits=1)`% |
| 10000 | `{julia} round(100 * C_mc[10000]; digits=1)` $\pm$ `{julia} round(100 * C_se[10000]; digits=1)`% |
:::
::::


# Further Notes

## More Advanced Monte Carlo Methods

"Simple" Monte Carlo analysis: assumes that we can readily sample independent and identically-distributed random variables. 

There are other methods for when distributions are hard to sample from or uncertainties aren't independent.

## On Random Number Generators

:::: {.columns}
::: {.column width=40%}
Random number generators are not *really* random, only **pseudorandom**.

This is why setting a seed is important. But even that can go wrong...  
:::
::: {.column width=60%}

![XKCD Cartoon 221: Random Number](https://imgs.xkcd.com/comics/random_number.png){width=90%}

::: {.caption}
Source: [XKCD #221](https://xkcd.com/221/)
:::
:::
::::

# Key Takeaways

## Key Takeaways

- Monte Carlo estimates are **unbiased**;
- Monte Carlo standard error is on the order $1/\sqrt{n}$, so not great if more direct approaches are available and tractable.
- Confidence intervals based on standard error.
- Need to decide what level of uncertainty is needed/meaningful for a given analysis.



# Upcoming Schedule

## Next Classes

**Wednesday**: Intro to Optimization

**Next Week**: Linear Programming (Monday)

## Assessments

**HW3**: Due Thursday (10/3) at 9pm

**Prelim 1**: Next Wednesday (10/9) in class, **includes material through today's lecture**.


